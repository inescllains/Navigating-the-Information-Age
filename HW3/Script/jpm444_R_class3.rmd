
Load a dataset
```{R}
library(tidyverse)

df <- read_tsv("/Users/jakubstauber/Library/CloudStorage/GoogleDrive-39536081@fsv.cuni.cz/Můj disk/Výuka/information_age/classes/class3/data/bbc-news-data.csv")
head(df)
```

Create Corpus
```{R}
library(quanteda)

corp <- corpus(df, text_field="content")

print(corp)
```

```{R}
summary(corp, 5)
```

```{R}
corp_sport <- corpus_subset(corp, category=="sport")
summary(corp_sport, 5)
```

Subset from Corpus
```{R}
corp_pol <- corpus_subset(corp, category %in% c("politics", "business"))
summary(corp_pol, 5)
```

Tokenization
```{R}
toks <- tokens(corp, 
        remove_punct = TRUE,
        remove_numbers = TRUE) %>%
        tokens_tolower() %>%
        tokens_wordstem() %>%
        tokens_remove(pattern = stopwords("en")) %>%
        tokens_remove(pattern = "1.13bn")

print(toks)
```

```{R}
toks_ngram <- tokens_ngrams(toks, n = 2:3)
print(toks_ngram)
```

DFM
```{R}
dfmat <- dfm(toks)
print(dfmat)
```

```{R}
library(quanteda.textmodels)
library(quanteda.textstats)
library(quanteda.textplots)

textstat_frequency(dfmat)
```


Counting Words

```{R}
dfm_category <- dfm(toks) %>%
              dfm_group(groups = category)
textstat_frequency(dfm_category, n=10, groups = category)
```

Counting Words - Visualisation

```{R}
data <- textstat_frequency(dfmat, n=10)

data %>% 
  ggplot(aes(x=reorder(feature, frequency), y=frequency))+
  geom_point()+
  coord_flip()+
  labs(x="")

```

Relative Frequencies

```{R}
dfmat_prop <- dfm_weight(dfmat, scheme  = "prop")
textstat_frequency(dfmat_prop, n=10)
```


TF-IDF

```{R}
dfmat_tfidf <- dfm_tfidf(dfm_category)
textstat_frequency(dfmat_tfidf,force=TRUE, n=10, groups = category)
```

Collocations Extraction
```{R}
colloc <- toks %>%
  textstat_collocations(min_count = 100)

print(colloc)
```

Collocations Extraction
```{R}
toks_update <- toks %>%
  tokens_compound(colloc, join=TRUE) %>%
  tokens_remove(pattern = c("mr", "said"))

dfm_update <- dfm(toks_update) %>%
              dfm_group(groups = category) %>%
              dfm_tfidf()
textstat_frequency(dfm_update, n=10,force=TRUE, groups = category)
```

Keywords in Context

```{R}
toks2 <- tokens(corp)

kw_blair <- kwic(toks2, pattern = c("blair"), window = 10)
head(kw_blair, 5)
```


Lexical Diversity
```{R}
tstat_lexdiv <- textstat_lexdiv(dfm_category, measure = c("TTR", "C"))
print(tstat_lexdiv)
```

Document Similarity
```{R}
simil <- textstat_simil(dfmat, method = "cosine", margin = "documents")
as.matrix(simil)
```

Document Similarity
```{R}
simil2 <- textstat_simil(dfmat, dfmat[, c("blair", "brown")], method = "cosine", margin = "features")
as.list(simil2, n = 10)
```